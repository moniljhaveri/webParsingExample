{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Houses in Redfin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monil Jhaveri \n",
    "29685415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To be completed INDIVIDUALLY and due on April 10 at 7pm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will practice web scraping. Let's get some basic information for each house/apartment in Massachusetts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see houses around Boston University [here](https://www.redfin.com/zipcode/02215)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Information to be scraped](redfin.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each house page, scrape the information of this [house](https://www.redfin.com/MA/Boston/120-Mountfort-St-02215/unit-502/home/11838719), including \n",
    "* full address\n",
    "* price\n",
    "* number of beds\n",
    "* number of baths (full or not)\n",
    "* Sq. Ft (square feet)\n",
    "* price per Sq. Ft\n",
    "* property type\n",
    "* County\n",
    "* community\n",
    "* built\n",
    "* Property details (e.g. dishwasher, elevator, parking, heating, air conditioning, maintenance, etc.)\n",
    "\n",
    "Try to extract as many features as possible. You will use this information in future homework. \n",
    "\n",
    "\n",
    "Save the data in \"house.csv\" in the following format:\n",
    "\n",
    "full address, price, beds, baths, ......\n",
    "\n",
    "**To receive credit, you must commit house.csv to Github.** **(20 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran into alot of trouble doing the redfin part of the assignment. I ended up getting blocked multiple times and my approach of getting all the links first was ultimately found to be a huge waste of time. I lost about two or three day doing that. My approach to this was to strape all the links and then work parsing the data because I felt that was easier. Ultimately, I should have parsed one page first instead of trying to get all the links first. I tried with requests and that worked for a little bit but that kept breaking. Then I tried with selenium and that worked but would quickly gave me a repeated captchas that I was unable to fix. \n",
    "\n",
    "This homework combined with the fact that my job got very busy over the same time period slashed with each other. So with time running out I moved onto the trip advisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import parser\n",
    "from lxml import html\n",
    "from time import sleep \n",
    "import csv\n",
    "from selenium import webdriver \n",
    "import random\n",
    "import pickle \n",
    "import sys\n",
    "import csv\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from random import choice, randint\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import PhantomJS\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-eb2c0ba1ed05>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-eb2c0ba1ed05>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    print(len(cleanLinks    ra\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_link_data(zipcode, driver):\n",
    "    url = \"https://www.redfin.com/zipcode/\" + zipcode\n",
    "    #r = requests.get(url, headers={'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'})\n",
    "    #root = 'http://www.redfin.com'\n",
    "    #driver = webdriver.Chrome()\n",
    "    #driver.get(url)\n",
    "\n",
    "    driver.get(url)\n",
    "    page_source = driver.page_source\n",
    "    #data = r.text\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    #print(soup)\n",
    "    zipDict = {} \n",
    "    links = [] \n",
    "    for i in soup.find_all(href=True):\n",
    "        links.append(i['href'])\n",
    "    links = set(links)\n",
    "    cleanLinks = [] \n",
    "    for i in links: \n",
    "        if(i[:4] == \"/MA/\"): \n",
    "            cleanLinks.append(i)\n",
    "    print(len(cleanLinks    ra\n",
    "    randtime = random.randint(10, 20)\n",
    "    sleep(randtime)))\n",
    "    \n",
    "    return cleanLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "linkList = get_link_data('01613', driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01001\n",
      "01002\n",
      "01003\n",
      "01004\n",
      "01005\n",
      "01007\n",
      "01008\n",
      "01009\n",
      "01010\n",
      "01011\n",
      "01012\n",
      "01013\n",
      "01014\n",
      "01020\n",
      "01021\n",
      "01022\n",
      "01026\n",
      "01027\n",
      "01028\n",
      "01029\n",
      "01030\n",
      "01031\n",
      "01032\n",
      "01033\n",
      "01034\n",
      "01035\n",
      "01036\n",
      "01037\n",
      "01038\n",
      "01039\n",
      "01040\n",
      "01041\n",
      "01050\n",
      "01053\n",
      "01054\n",
      "01056\n",
      "01057\n",
      "01059\n",
      "01060\n",
      "01061\n",
      "01062\n",
      "01063\n",
      "01066\n",
      "01068\n",
      "01069\n",
      "01070\n",
      "01071\n",
      "01072\n",
      "01073\n",
      "01074\n",
      "01075\n",
      "01077\n",
      "01079\n",
      "01080\n",
      "01081\n",
      "01082\n",
      "01083\n",
      "01084\n",
      "01085\n",
      "01086\n",
      "01088\n",
      "01089\n",
      "01090\n",
      "01092\n",
      "01093\n",
      "01094\n",
      "01095\n",
      "01096\n",
      "01097\n",
      "01098\n",
      "01101\n",
      "01102\n",
      "01103\n",
      "01104\n",
      "01105\n",
      "01106\n",
      "01107\n",
      "01108\n",
      "01109\n",
      "01111\n",
      "01115\n",
      "01116\n",
      "01118\n",
      "01119\n",
      "01128\n",
      "01129\n",
      "01133\n",
      "01138\n",
      "01139\n",
      "01144\n",
      "01151\n",
      "01152\n",
      "01195\n",
      "01199\n",
      "01201\n",
      "01202\n",
      "01203\n",
      "01220\n",
      "01222\n",
      "01223\n",
      "01224\n",
      "01225\n",
      "01226\n",
      "01227\n",
      "01229\n",
      "01230\n",
      "01235\n",
      "01236\n",
      "01237\n",
      "01238\n",
      "01240\n",
      "01242\n",
      "01243\n",
      "01244\n",
      "01245\n",
      "01247\n",
      "01252\n",
      "01253\n",
      "01254\n",
      "01255\n",
      "01256\n",
      "01257\n",
      "01258\n",
      "01259\n",
      "0\n",
      "01260\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-26aa4784e40a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mzipDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mzipcode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_link_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mzipDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"hw4_list5.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#print(random.randint(2,5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-ae2fc191f639>\u001b[0m in \u001b[0;36mget_link_data\u001b[0;34m(zipcode, driver)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#driver.get(url)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrandtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpage_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "zipfiles = open('zipcode.csv', 'r')\n",
    "reader = csv.reader(zipfiles)\n",
    "zipDict = {}\n",
    "driver = webdriver.Chrome()\n",
    "count = 0 \n",
    "for i in reader:\n",
    "    zipcode = i[-1]\n",
    "    print(zipcode)\n",
    "    #test = zipcode\n",
    "    if (zipcode == '01259'):\n",
    "        count += 1\n",
    "    if (count != 0):\n",
    "        zipDict[zipcode] = get_link_data(zipcode, driver)\n",
    "        pickle.dump( zipDict, open( \"hw4_list5.p\", \"wb\" ) )\n",
    "#print(random.randint(2,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "loa = pickle.load(open(\"hw4_list2.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01075': [], '01077': [], '01079': [], '01080': [], '01081': [], '01082': ['/MA/Hardwick/1234-Petersham-Rd-01031/home/110300376', '/MA/Hardwick/68-North-St-01031/unit-68/home/22062739', '/MA/Hardwick/70-North-ST-01031/unit-70/home/95084420', '/MA/Hardwick/838-Muddy-Brook-Rd-01031/home/14814453', '/MA/Hardwick/2692-Greenwich-Rd-01031/home/16517535', '/MA/Hardwick/2813-Greenwich-Rd-01031/home/16517161'], '01083': ['/MA/Warren/77-O-Neil-Rd-01083/home/112923886', '/MA/Warren/261-Dunham-Rd-01083/home/112861240', '/MA/Warren/12-Presidential-Cir-01083/home/108243152', '/MA/Warren/810-Little-Rest-Rd-01083/home/108497386', '/MA/Warren/26-Crescent-St-01083/home/110298612', '/MA/Warren/39-Couture-Dr-01083/home/105664744', '/MA/Warren/49-Constitution-Ave-01083/home/103634239', '/MA/Warren/142-144-Maple-St-01083/home/113242220', '/MA/Warren/0-W-Old-Warren-Rd-01083/home/17541818', '/MA/Warren/1355-Bemis-Rd-01010/home/81205188', '/MA/Warren/Lot-1-Brimfield-Rd-01083/home/112872343', '/MA/Warren/72-Pleasant-St-01083/home/16629063', '/MA/Warren/234-W-Old-Warren-Rd-01083/home/16628693', '/MA/Warren/796-Reed-St-01083/home/112977665', '/MA/Warren/1493-Southbridge-Rd-01083/home/109000034', '/MA/Warren/33-Carl-St-01083/home/108546359', '/MA/Warren/0-Brimfield-Rd-01083/home/105516613', '/MA/Warren/0-Cronin-Rd-01083/unit-4/home/112877961', '/MA/Warren/0-Cronin-Rd-L-01083/home/103639566', '/MA/Warren/Lot-2-Brimfield-Rd-01083/home/112872352', '/MA/Warren/lot-155-Bemis-Rd-01083/home/108372044', '/MA/Warren/356-Ware-01083/home/105504006', '/MA/Warren/153-Crouch-Rd-01083/home/113205747', '/MA/Warren/0-Brimfield-01083/home/105504024', '/MA/Warren/0-Cronin-Rd-01083/home/103639567', '/MA/Warren/235-Southbridge-Rd-01083/home/39937364', '/MA/Warren/0-Cronin-Rd-01083/unit-1/home/108991727', '/MA/Warren/24-Southbridge-Road-Ext-01083/home/113042162', '/MA/Warren/0-Cronin-Rd-01083/unit-7/home/108991740', '/MA/Warren/680-Brimfield-Rd-01083/home/103737804', '/MA/Warren/0-Cronin-Rd-01083/unit-3/home/112877901', '/MA/Warren/Main-St-01083/home/16628497', '/MA/Warren/0-Cronin-Rd-01083/unit-2/home/112877892'], '01084': [], '01085': [], '01086': [], '01088': [], '01089': [], '01090': [], '01092': [], '01093': [], '01094': [], '01095': [], '01096': [], '01097': [], '01098': [], '01101': [], '01102': [], '01103': [], '01104': [], '01105': [], '01106': [], '01107': [], '01108': [], '01109': [], '01111': [], '01115': [], '01116': [], '01118': [], '01119': [], '01128': [], '01129': [], '01133': [], '01138': [], '01139': [], '01144': [], '01151': [], '01152': [], '01195': [], '01199': [], '01201': [], '01202': [], '01203': [], '01220': [], '01222': [], '01223': [], '01224': [], '01225': [], '01226': [], '01227': [], '01229': [], '01230': [], '01235': [], '01236': [], '01237': [], '01238': [], '01240': [], '01242': [], '01243': [], '01244': [], '01245': [], '01247': [], '01252': [], '01253': [], '01254': [], '01255': [], '01256': [], '01257': [], '01258': []}\n"
     ]
    }
   ],
   "source": [
    "print(loa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-01caaefd195c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0msearch_zip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"02215\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mprop_search_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_listings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchrome_driver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_zip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mprop_search_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-01caaefd195c>\u001b[0m in \u001b[0;36mget_listings\u001b[0;34m(driver, search_zip)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mpage_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mproperty_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_property_urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\u002F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mproperty_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperty_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperty_urls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "\n",
    "# from pyvirtualdisplay import Display\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "reg_property_urls = re.compile('(/[A-Z][A-Z]/[A-Za-z\\-/0-9]+/home/[0-9]+)')\n",
    "\n",
    "## set user-agent headers when testing using \"requests\"\n",
    "# user_agent= 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36'\n",
    "# header ={'User-Agent': user_agent}\n",
    "\n",
    "# display.start()\n",
    "chrome_driver = Chrome()\n",
    "\n",
    "    \n",
    "## define overall url and create empty prop list to store search results    \n",
    "redfin_url = \"https://www.redfin.com\"\n",
    "prop_search_results = []\n",
    "\n",
    "# display.stop()\n",
    "\n",
    "## function to build phantomjs driver with unique rotating headers; headless driver\n",
    "\n",
    "\n",
    "## set proxy info if needed\n",
    "# service_args = [\n",
    "#         '--proxy=192.168.56.2:1989',\n",
    "#         '--proxy-type=http',\n",
    "#         '--ignore-ssl-errors=true'\n",
    "#         ]\n",
    "\n",
    "# phjs_driver = create_phantomjs_driver()\n",
    "\n",
    "# phjs_driver = create_phantomjs_driver(service_args=service_args)\n",
    "# phjs_driver.quit()\n",
    "\n",
    "def get_listings(driver, search_zip):\n",
    "    zip_url = redfin_url + \"/zipcode/\" + search_zip\n",
    "\n",
    "    property_urls = reg_property_urls.findall(page_source.replace('\\\\u002F', '/'))\n",
    "    property_urls = list(set(property_urls))\n",
    "    if len(property_urls) > 0:\n",
    "        print('There are ' + str(len(property_urls)) + ' listings for ' + search_zip)\n",
    "    sleep(randint(2,5))\n",
    "    return property_urls\n",
    "print(\"Done\")\n",
    "\n",
    "search_zip = \"02215\"\n",
    "prop_search_results.extend(get_listings(chrome_driver, search_zip))\n",
    "prop_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "driver = webdriver.Chrome() \n",
    "driver.get('http://www.google.com')\n",
    "print(driver.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "#driver = webdriver.Chrome() \n",
    "root = 'http://www.redfin.com'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "data = {}\n",
    "for i in linkList:\n",
    "    url = root + i \n",
    "    driver.get(url)\n",
    "    x = driver.page_source\n",
    "    data[i] = parsePage(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump( data, open( \"hw4_q1data.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsePage(x): \n",
    "    page = BeautifulSoup(x, 'lxml') \n",
    "    #print(page.prettify)\n",
    "    #for i in page.find_all('div'): \n",
    "        #classL = i.get('class')\n",
    "        #print(classL)\n",
    "        #print(i.get_text())\n",
    "        #if classL is not None:\n",
    "        #    if 'top-stats' in classL: \n",
    "        #        pass\n",
    "                #print(i)\n",
    "    dataDict = {}\n",
    "    #dataDict[x] = []\n",
    "    topStats = []\n",
    "    descriptionList = [] \n",
    "    for i in page.find('div', {'class': 'top-stats'}):\n",
    "        topStats.append(i.get_text())\n",
    "    for i in page.find('div', {'class':'descriptive-paragraph'}):\n",
    "        descriptionList.append(i.get_text())\n",
    "    dataDict['topStat'] = topStats\n",
    "    dataDict['description'] = descriptionList\n",
    "    dataDict['source'] = x\n",
    "    return dataDict \n",
    "        #pass\n",
    "        #print(i.get_text())\n",
    "        #print(i.get('itemprop'))\n",
    "#parsePage(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can scrape and extract information of a house, scrape the information of houses in Massachusetts (as of today there are around 10000 houses in MA listed in Redfin). To see all the listings, you can query all the zip codes in Massachusetts. You can see all the zip codes in Massachusetts [here](https://github.com/evimaria/CS506-Spring2007/blob/master/Homeworks/zipcodes.txt).\n",
    "Save the data in \"MA_houses.csv\", one line for each house:\n",
    "**To receive credit, you must commit MA_houses.csv to Github** **(20 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many websites will block you if you are sending too many queries. To prevent getting block, you should wait between queries (typically 2-5 seconds). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
